{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** This notebook is used to implement a classifier to devide \"SPAM\" and \"no SPAM\" messages. The dataset `SMS Spam Collection v. 1` can be found ([here](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/)).\n",
    "\n",
    "**Project Name:** Natural Language SPAM Classifier\n",
    "\n",
    "**Author:** Silas Mederer\n",
    "\n",
    "**Date:** 2020-12-09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import nltk\n",
    "\n",
    "# warnings handler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                               text\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/SMSSpamCollection.txt\", encoding=\"utf-8\", header=None, delimiter=\"\\t\", names=[\"target\", \"text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm gonna be home soon and i don't want to tal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    target                                               text\n",
       "0        0  Go until jurong point, crazy.. Available only ...\n",
       "1        0                      Ok lar... Joking wif u oni...\n",
       "2        1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3        0  U dun say so early hor... U c already then say...\n",
       "4        0  Nah I don't think he goes to usf, he lives aro...\n",
       "5        1  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6        0  Even my brother is not like to speak with me. ...\n",
       "7        0  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8        1  WINNER!! As a valued network customer you have...\n",
       "9        1  Had your mobile 11 months or more? U R entitle...\n",
       "10       0  I'm gonna be home soon and i don't want to tal..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding target variable\n",
    "df[\"target\"] = np.where(df[\"target\"] == \"spam\", 1, 0)\n",
    "df.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 5572 instances of 2 variables.\n",
      "It contains 747 spam messages (13.4% of all)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset contains {} instances of {} variables.\".format(df.shape[0], df.shape[1]))\n",
    "\n",
    "print(\n",
    "    \"It contains {} spam messages ({:.1%} of all)\".format(\n",
    "        df[df.target == 1].shape[0],\n",
    "        df[df.target == 1].shape[0] / df.shape[0],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 'no SPAM'\n",
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "Ok lar... Joking wif u oni...\n",
      "\n",
      "Example 'SPAM'\n",
      "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
      "WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExample 'no SPAM'\")\n",
    "print(df.text[0])\n",
    "print(df.text[1])\n",
    "print(\"\\nExample 'SPAM'\")\n",
    "print(df.text[2])\n",
    "print(df.text[5])\n",
    "print(df.text[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like SPAM messages start with wordslike \"winner\" or \"free\". Let´s keep this in mind while evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam classification\n",
    "\n",
    "The model will be a \"vanilla\" classifier, without pouring too many thoughts about what the actual messages, spam or not, look like. To improve your model you can of course have a closer look and investigate the data more in detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset between train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"target\"], random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by using a count vectorizer on our data. It will convert our text and return a sparse matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vectorized: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<4179x7523 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 56353 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(X_train)\n",
    "\n",
    "# transform the documents in the training data to a document-term matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "print(\"X_train_vectorized: \")\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape     = 4179\n",
      "Vocabulary length = 7523\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape     = {}\".format(X_train.shape[0]))\n",
    "print(\"Vocabulary length = {}\".format(len(vect.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1500)\n",
    "log_reg.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Train KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Train SVC\n",
    "svc = LinearSVC()\n",
    "svc.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "log_reg_predictions = log_reg.predict(vect.transform(X_test))\n",
    "knn_predictions = knn.predict(vect.transform(X_test))\n",
    "svc_predictions = svc.predict(vect.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression AUC = 0.917\n",
      "KNN                AUC = 0.822\n",
      "SVC                AUC = 0.935\n"
     ]
    }
   ],
   "source": [
    "print(f\"LogisticRegression AUC = {round(roc_auc_score(y_test, log_reg_predictions),3)}\")\n",
    "print(f\"KNN                AUC = {round(roc_auc_score(y_test, knn_predictions),3)}\")\n",
    "print(f\"SVC                AUC = {round(roc_auc_score(y_test, svc_predictions),3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model is a linear support vector classifier with an AUC of 0.935, second best is a logistic regression with AUC 0.917."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Reg features:\n",
      "\n",
      "Smallest Coefs:\n",
      "['my' 'but' 'gt' 'lt' 'sir' 'fullonsms' 'him' 'me' 'can' 'place']\n",
      "\n",
      "Largest Coefs: \n",
      "['txt' 'text' 'call' 'ringtone' 'reply' 'chat' 'won' 'uk' '150p' 'stop']\n",
      "\n",
      "SVC features:\n",
      "\n",
      "Smallest Coefs:\n",
      "['liked' 'place' 'sir' 'once' 'fullonsms' 'gt' 'can' 'lunch' 'lt' 'missed']\n",
      "\n",
      "Largest Coefs: \n",
      "['146tf150p' 'ringtoneking' '84484' 'ringtone' 'stories' 'filthy' 'txt'\n",
      " '88066' 'won' 'chat']\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "log_reg_sorted_coef_index = log_reg.coef_[0].argsort()\n",
    "print(\"Log Reg features:\\n\")\n",
    "print(\"Smallest Coefs:\\n{}\\n\".format(feature_names[log_reg_sorted_coef_index[:10]]))\n",
    "print(\"Largest Coefs: \\n{}\".format(feature_names[log_reg_sorted_coef_index[:-11:-1]]))\n",
    "\n",
    "svc_sorted_coef_index = svc.coef_[0].argsort()\n",
    "print(\"\\n\" + \"SVC features:\\n\")\n",
    "print(\"Smallest Coefs:\\n{}\\n\".format(feature_names[svc_sorted_coef_index[:10]]))\n",
    "print(\"Largest Coefs: \\n{}\".format(feature_names[svc_sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vectorized: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<4179x2308 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 49944 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(min_df=3).fit(X_train)\n",
    "\n",
    "# transform the documents in the training data to a document-term matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "print(\"X_train_vectorized: \")\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1500)\n",
    "log_reg.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Train KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Train SVC\n",
    "svc = LinearSVC()\n",
    "svc.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "log_reg_predictions = log_reg.predict(vect.transform(X_test))\n",
    "knn_predictions = knn.predict(vect.transform(X_test))\n",
    "svc_predictions = svc.predict(vect.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression AUC = 0.876\n",
      "KNN                AUC = 0.822\n",
      "SVC                AUC = 0.944\n"
     ]
    }
   ],
   "source": [
    "print(f\"LogisticRegression AUC = {round(roc_auc_score(y_test, log_reg_predictions),3)}\")\n",
    "print(f\"KNN                AUC = {round(roc_auc_score(y_test, knn_predictions),3)}\")\n",
    "print(f\"SVC                AUC = {round(roc_auc_score(y_test, svc_predictions),3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVC is the best and increased to an AUC score of 0.944. Both other models performances decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Reg features:\n",
      "\n",
      "Smallest Coefs:\n",
      "['beautiful' 'blacko' '2geva' 'aroundn' 'costs' 'barolla' 'alive' 'bought'\n",
      " '2stoptx' 'anything']\n",
      "\n",
      "Largest Coefs: \n",
      "['cutest' '2morrow' 'corect' 'admit' 'bergkamp' 'competition' 'director'\n",
      " 'cricketer' 'casting' 'advisors']\n",
      "\n",
      "SVC features:\n",
      "\n",
      "Smallest Coefs:\n",
      "['affection' 'beautiful' '2stoptx' 'alive' 'desparate' 'b4280703'\n",
      " 'barolla' '2geva' 'blacko' 'bsnl']\n",
      "\n",
      "Largest Coefs: \n",
      "['cutest' 'corect' 'da' '08000930705' '382' 'bergkamp' 'director' 'child'\n",
      " '402' 'celebration']\n"
     ]
    }
   ],
   "source": [
    "# Sort the coefficients from the model\n",
    "log_reg_sorted_coef_index = log_reg.coef_[0].argsort()\n",
    "print(\"Log Reg features:\\n\")\n",
    "print(\"Smallest Coefs:\\n{}\\n\".format(feature_names[log_reg_sorted_coef_index[:10]]))\n",
    "print(\"Largest Coefs: \\n{}\".format(feature_names[log_reg_sorted_coef_index[:-11:-1]]))\n",
    "\n",
    "svc_sorted_coef_index = svc.coef_[0].argsort()\n",
    "print(\"\\n\" + \"SVC features:\\n\")\n",
    "print(\"Smallest Coefs:\\n{}\\n\".format(feature_names[svc_sorted_coef_index[:10]]))\n",
    "print(\"Largest Coefs: \\n{}\".format(feature_names[svc_sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "It is also possible to use stemming (remove morphological affixes from words, leaving only the word stem) as a preprocessing step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing stemmer and countvectorizer \n",
    "stemmer = nltk.PorterStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "stemm_vectorizer = CountVectorizer(analyzer=stemmed_words)\n",
    "\n",
    "# Transform X_train\n",
    "X_train_stemm_vectorized = stemm_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "log_reg_stemm = LogisticRegression(max_iter=1500)\n",
    "log_reg_stemm.fit(X_train_stemm_vectorized, y_train)\n",
    "\n",
    "# Train SVC\n",
    "svc_stemm = LinearSVC()\n",
    "svc_stemm.fit(X_train_stemm_vectorized, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "log_reg_predictions = log_reg_stemm.predict(stemm_vectorizer.transform(X_test))\n",
    "svc_predictions = svc_stemm.predict(stemm_vectorizer.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression AUC = 0.93\n",
      "SVC                AUC = 0.951\n"
     ]
    }
   ],
   "source": [
    "print(f\"LogisticRegression AUC = {round(roc_auc_score(y_test, log_reg_predictions),3)}\")\n",
    "print(f\"SVC                AUC = {round(roc_auc_score(y_test, svc_predictions),3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming improved the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "The same way we used stemming we can also apply lemmatization (grouping together the inflected forms of a word) to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def lemmatize_word(doc):\n",
    "    return (WNlemma.lemmatize(t) for t in analyzer(doc))\n",
    "\n",
    "lemm_vectorizer = CountVectorizer(analyzer=lemmatize_word)\n",
    "\n",
    "# Transform X_train\n",
    "X_train_lemm_vectorized = lemm_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "log_reg_lemm = LogisticRegression(max_iter=1500)\n",
    "log_reg_lemm.fit(X_train_lemm_vectorized, y_train)\n",
    "\n",
    "# Train SVC\n",
    "svc_lemm = LinearSVC()\n",
    "svc_lemm.fit(X_train_lemm_vectorized, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "log_reg_predictions = log_reg_lemm.predict(lemm_vectorizer.transform(X_test))\n",
    "svc_predictions = svc_lemm.predict(lemm_vectorizer.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression AUC = 0.92\n",
      "SVC                AUC = 0.938\n"
     ]
    }
   ],
   "source": [
    "print(f\"LogisticRegression AUC = {round(roc_auc_score(y_test, log_reg_predictions),3)}\")\n",
    "print(f\"SVC                AUC = {round(roc_auc_score(y_test, svc_predictions),3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization did not work in this case. Maybe I need to research more lingustics. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was able to build a model (SVC with stemming) that scored an AUC of 0.951. But we need to keep in mind that the dataset was higly imbalanced. The tought we started with, that the word \"winner\" or \"free\" has a hig impact, has not been proofed jet. We just know that these are none of the top ten words. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
